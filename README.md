### Introduction

------

This repository contains the code for *Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model*.

### Requirements and Installations

------

- Pytorch version >= 1.4.0

- Python version >= 3.6

  ```
  %cd PrLM-for-generation-tasks/fairseq-master
  !python -m pip install --editable .
  ```

  

- Our implemention environment: 

  - Google Colab Pro (Python Python 3.6.9 on linux, Pytorch 1.7.0+cu101)
  - a single NVIDIA Tesla V100 GPU (16130MiB)

- Requirements for 10 different pre-trained checkpoints used in our paper

  - If you use KoBERT, ELECTRA base, ELECTRA small, DistilKoBERT, Multi-lingual BERT cased, Multi-lingual BERT uncased, XLM RoBERTa base, and XLM RoBERTa large, install the library as follows:

    ```
    # in Google Colab
    
    !pip install transformers==4.1.1
    !pip install sentencepiece
    ```

  - If you use HanBERT, install the library as follows:

    ```
    # in Google Colab
    # first, download https://drive.google.com/file/d/1LUyrnhuNC3e8oD2QMJv8tIDrXrxzmdu4/view to google drive
    
    import os
    !pip install transformers==2.2.2
    !git clone https://github.com/monologg/HanBert-Transformers.git
    !tar -xf drive/MyDrive/HanBert-54kN-torch.tar 
    !cp HanBert-Transformers/tokenization_hanbert.py 
    ```

  - If you use KoGPT2, install the library as follows:

    ```
    # in Google Colab
    
    !git clone https://github.com/SKT-AI/KoGPT2.git
    !rm KoGPT2/requirements.txt
    !cp PrLM-for-generation-tasks/fairseq-master/kogpt_requirement.txt /KoGPT2/
    !rm KoGPT2/kogpt2/pytorch_kogpt2.py
    !cp PrLM-for-generation-tasks/fairseq-master/pytorch_kogpt2.py KoGPT2/kogpt2/
    %cd KoGPT2
    !pip install -r KoGPT2/kogpt_requirement.txt
    !pip install .
    %cd ..
    ```

### Simple Tutorial

------



#### Data preprocessing

First, you should prepare tokenized files for your generation tasks:

```
train.src valid.src test.src train.tgt valid.tgt test.tgt
```

Then rename original files (non-tokenized or non-bpe) to `xx.prlm` like:

```
train.prlm valid.prlm test.prlm
```

These `xx.prlm` files are processed through our `preprocess.py` according to each PrLM's configurations (i.e., word segmentation rules, vocab size).

```
!python PrLM-for-generation-tasks/fairseq-master/preprocess.py \
--source-lang srt --target-lang tgt --trainpref $TEXT/train \
--validpref $TEXT/valid \
--testpref $TEXT/test \
--prlm-suffix $prlmsuffix \
--destdir $TEXT/destdir \
--joined-dictionary --workers 1
```

The additional options we add:

```
group.add_argument("--prlm-suffix", metavar="FP", default=None, help="prlm file suffix")
```

For `--prlm-suffix` , you have 10 options like:

```
KoBERT: "kobert"
HanBERT: "han_bert"
DistilKoBERT: "distilkobert"
KoGPT2: "kogpt2"
ELECTRA base: "electra_base_v3"
ELECTRA small: "electra_small_v3"
Multi-lingual BERT cased: "bert_base_multilingual_cased"
Multi-lingual BERT uncased: "bert_base_multilingual_uncased"
XLM RoBERTa base:"xlm_roberta_base"
XLM RoBERTa large: "xlm_roberta_large" 
```



#### Model Train

Train a Transformer model with 6 stacked encoder and decoder-layers.

```
!python PrLM-for-generation-tasks/fairseq-master/train.py \
    $TEXT/destdir --arch transformer --share-all-embeddings \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \
    --lr 0.0007 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \
    --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0\
    --max-tokens 3500 --label-smoothing 0.1 --save-dir savedir --log-interval 200 --max-epoch 50 \
    --keep-interval-updates -1 --save-interval-updates 0 --criterion label_smoothed_cross_entropy --update-freq 8  \
    --load-prlm --add-prlm $prlmsuffix --merging-method $mergingmethod  --where-add-connection $whereaddconnection \
    --posit-add-prlm $positaddprlm --which-layer-from-prlm $whichlayerfromprlm \
```

The additional options we add:

```
parser.add_argument('--merging-method', default=None, help='how to merge prlm with original')           
parser.add_argument('--posit-add-prlm', default=None, help='position of applying prlm')
parser.add_argument('--which-layer-from-prlm', type=int, default=-1, help='which-layer-from-prlm')
parser.add_argument('--where-add-connection', default=None, help='--where-add-connection')
parser.add_argument('--add-prlm', default=None, help='add prlm')
```

For `--merging-method` , you have 4 options like:

```
Using summation: "add"
Using gate network: "gate"
Using average: "average"
Using dropnet: "dropnet"
```

For `--posit-add-prlm` , you have 3 options like:

```
Adding PrLM-dedicated modules to encoder: "src"
Adding PrLM-dedicated modules to decoder: "tgt"
Adding PrLM-dedicated modules to both: "all"
```

For `--where-add-connection` , you have 4 options like:

```
Merging the PrLM flow with the source input flows after SelfAttn: "self"
Merging the PrLM flow with the source input flows after First Add&Norm: "self_norm"
Merging the PrLM flow with the source input flows after FFN: "ffn"
Merging the PrLM flow with the source input flows after Second Add&Norm: "ffn_norm"
```

For `--which-layer-from-prlm` , you can select the intermediate hidden state of the PrLM. 

Example, `-2` denotes second-to-last hidden state, and `-4` denotes forth-to-last hidden state.



#### Generate

generate inference using the trained model.

```
!python PrLM-for-generation-tasks/fairseq-master/generate.py \
    $TEXT/destdir --gen-subset test --load-prlm --add-prlm $prlmsuffix \
    --source-lang src --target-lang tgt \
    --path $modelcheckpointpath  --beam 5 --nbest 1 --quiet 
```

