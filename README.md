### Introduction

------

This repository contains the code for *Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model*.

Abstract

> This work empirically explores effective exploiting of intermediate output from pre-trained language models (PrLMs) for language generation tasks. For this purpose, we propose an improved method to integrate public checkpoints of PrLMs for the most convenience and perform extensive experiments on 6 different kinds of PrLMs, including BERT, ELECTRA, GPT2, Multi-lingual BERT, and XLM RoBERTa.Evaluation with automatic metrics shows that our approach significantly improves the generation quality on the generation tasks, up to 1.8 BLEU points for neural machine translation (Korean-to-English, Korean-to-Chinese) and 1.8 ROUGE points improvements for text summarization.


### Requirements and Installations

------

- Our implemention environment: 

  - Google Colab Pro (Python Python 3.6.9 on linux, Pytorch 1.7.0+cu101)
  - a single NVIDIA Tesla V100 GPU (16130MiB)

- Requirtments 
  - Pytorch version >= 1.4.0
  - Python version >= 3.6
  - Installation (in Google Colab)
    ```
    !git clone https://github.com/tmtmaj/Exploiting-PrLM-for-NLG-tasks.git
    %cd /content/Exploiting-PrLM-for-NLG-tasks 
    !python -m pip install --editable .
    %cd ..
    ```

 

- Requirements for 10 different pre-trained checkpoints used in our paper

  - If you use KoBERT, ELECTRA base, ELECTRA small, DistilKoBERT, Multi-lingual BERT cased, Multi-lingual BERT uncased, XLM RoBERTa base, and XLM RoBERTa large, install the library as follows:

    ```
    # in Google Colab
    
    !pip install transformers==4.1.1
    !pip install sentencepiece
    ```

  - If you use HanBERT, install the library as follows:
    - First, you have to download `HanBert-54kN-torch.tar` to your google drive from [URL](https://drive.google.com/open?id=1LUyrnhuNC3e8oD2QMJv8tIDrXrxzmdu4)

    ```
    # in Google Colab
    
    import os
    !pip install transformers==2.2.2
    !git clone https://github.com/monologg/HanBert-Transformers.git
    !tar -xf drive/MyDrive/HanBert-54kN-torch.tar 
    !cp /content/HanBert-Transformers/tokenization_hanbert.py /content/Exploiting-PrLM-for-NLG-tasks
    ```

  - If you use KoGPT2, install the library as follows:

    ```
    # in Google Colab
    
    !git clone https://github.com/SKT-AI/KoGPT2.git
    %cd /content/KoGPT2/
    !git reset --hard 7fe98b54cf2c12ab6ba7fac1e1aa7c87e93790c4
    %cd ..
    !rm KoGPT2/requirements.txt
    !cp PrLM-for-generation-tasks/fairseq-master/kogpt_requirement.txt /KoGPT2/
    !rm KoGPT2/kogpt2/pytorch_kogpt2.py
    !cp PrLM-for-generation-tasks/fairseq-master/pytorch_kogpt2.py KoGPT2/kogpt2/
    %cd KoGPT2
    !pip install -r KoGPT2/kogpt_requirement.txt
    !pip install .
    %cd ..
    ```

### Simple Tutorial

------



#### Data preprocessing

First, you should prepare tokenized files for your generation tasks:

```
train.src valid.src test.src train.tgt valid.tgt test.tgt
```

Then rename original files (non-tokenized or non-bpe) to `*.prlm` like:

```
train.prlm valid.prlm test.prlm
```

These `*.prlm` files are processed through our `preprocess.py` according to each PrLM's configurations (i.e., word segmentation rules, vocab size).

```
!python PrLM-for-generation-tasks/fairseq-master/preprocess.py \
--source-lang srt --target-lang tgt --trainpref $TEXT/train \
--validpref $TEXT/valid \
--testpref $TEXT/test \
--prlm-suffix $prlmsuffix \
--destdir $TEXT/destdir \
--joined-dictionary --workers 1
```

The additional options we add:

```
group.add_argument("--prlm-suffix", metavar="FP", default=None, help="prlm file suffix")
```

For `--prlm-suffix` , you have 10 options like:

```
KoBERT: "kobert"
HanBERT: "han_bert"
DistilKoBERT: "distilkobert"
KoGPT2: "kogpt2"
ELECTRA base: "electra_base_v3"
ELECTRA small: "electra_small_v3"
Multi-lingual BERT cased: "bert_base_multilingual_cased"
Multi-lingual BERT uncased: "bert_base_multilingual_uncased"
XLM RoBERTa base:"xlm_roberta_base"
XLM RoBERTa large: "xlm_roberta_large" 
```



#### Model Train

Train a Transformer model with 6 stacked encoder and decoder-layers.

```
!python PrLM-for-generation-tasks/fairseq-master/train.py \
    $TEXT/destdir --arch transformer --share-all-embeddings \
    --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \
    --lr 0.0007 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \
    --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0\
    --max-tokens 3500 --label-smoothing 0.1 --save-dir savedir --log-interval 200 --max-epoch 50 \
    --keep-interval-updates -1 --save-interval-updates 0 --criterion label_smoothed_cross_entropy --update-freq 8  \
    --load-prlm --add-prlm $prlmsuffix --merging-method $mergingmethod  --where-add-connection $whereaddconnection \
    --posit-add-prlm $positaddprlm --which-layer-from-prlm $whichlayerfromprlm \
```

The additional options we add:

```
parser.add_argument('--merging-method', default=None, help='how to merge prlm with original')           
parser.add_argument('--posit-add-prlm', default=None, help='position of applying prlm')
parser.add_argument('--which-layer-from-prlm', type=int, default=-1, help='which-layer-from-prlm')
parser.add_argument('--where-add-connection', default=None, help='--where-add-connection')
parser.add_argument('--add-prlm', default=None, help='add prlm')
```

For `--merging-method` , you have 4 options like:

```
Using summation: "add"
Using gate network: "gate"
Using average: "average"
Using dropnet: "dropnet"
```

For `--posit-add-prlm` , you have 3 options like:

```
Adding PrLM-dedicated modules to encoder: "src"
Adding PrLM-dedicated modules to decoder: "tgt"
Adding PrLM-dedicated modules to both: "all"
```

For `--where-add-connection` , you have 4 options like:

```
Merging the PrLM flow with the source input flows after SelfAttn: "self"
Merging the PrLM flow with the source input flows after First Add&Norm: "self_norm"
Merging the PrLM flow with the source input flows after FFN: "ffn"
Merging the PrLM flow with the source input flows after Second Add&Norm: "ffn_norm"
```

For `--which-layer-from-prlm` , you can select the intermediate hidden state of the PrLM. 

Example, `-2` denotes second-to-last hidden state, and `-4` denotes forth-to-last hidden state.



#### Generate

generate inference using the trained model.

```
!python PrLM-for-generation-tasks/fairseq-master/generate.py \
    $TEXT/destdir --gen-subset test --load-prlm --add-prlm $prlmsuffix \
    --source-lang src --target-lang tgt \
    --path $modelcheckpointpath  --beam 5 --nbest 1 --quiet 
```

