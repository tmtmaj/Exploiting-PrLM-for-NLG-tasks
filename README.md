### Introduction

------

This repository contains the code for *Enhancing Language Generation with Effective Checkpoints of Pre-trained Language Model*.

Abstract

> This work empirically explores effective exploiting of intermediate output from pre-trained language models (PrLMs) for language generation tasks. For this purpose, we propose an improved method to integrate public checkpoints of PrLMs for the most convenience and perform extensive experiments on 6 different kinds of PrLMs, including BERT, ELECTRA, GPT2, Multi-lingual BERT, and XLM RoBERTa.Evaluation with automatic metrics shows that our approach significantly improves the generation quality on the generation tasks, up to 1.8 BLEU points for neural machine translation (Korean-to-English, Korean-to-Chinese) and 1.8 ROUGE points improvements for text summarization.


### Requirements and Installations

------

- Our implemention environment: 

  - Google Colab Pro (Python Python 3.6.9 on linux, Pytorch 1.7.0+cu101)
  - a single NVIDIA Tesla V100 GPU (16130MiB)

- Requirtments 
  - Pytorch version >= 1.4.0
  - Python version >= 3.6
  - Installation (in Google Colab)
    ```
    !git clone https://github.com/tmtmaj/Exploiting-PrLM-for-NLG-tasks.git
    %cd /content/Exploiting-PrLM-for-NLG-tasks 
    !python -m pip install --editable .
    %cd /content/
    ```

 

- Requirements for 10 different pre-trained checkpoints used in our paper

  - If you use KoBERT, ELECTRA base, ELECTRA small, DistilKoBERT, Multi-lingual BERT cased, Multi-lingual BERT uncased, XLM RoBERTa base, and XLM RoBERTa large, install the library as follows:

    ```
    # in Google Colab
    
    !pip install transformers==4.1.1
    !pip install sentencepiece
    ```

  - If you use HanBERT, install the library as follows:
    - First, you have to download `HanBert-54kN-torch.tar` to your google drive from [URL](https://drive.google.com/open?id=1LUyrnhuNC3e8oD2QMJv8tIDrXrxzmdu4)

    ```
    # in Google Colab
    
    import os
    !pip install transformers==2.2.2
    !git clone https://github.com/monologg/HanBert-Transformers.git
    !tar -xf drive/MyDrive/HanBert-54kN-torch.tar 
    !cp /content/HanBert-Transformers/tokenization_hanbert.py /content/Exploiting-PrLM-for-NLG-tasks
    ```

  - If you use KoGPT2, install the library as follows:

    ```
    # in Google Colab
    
    !git clone https://github.com/SKT-AI/KoGPT2.git
    %cd /content/KoGPT2/
    !git reset --hard 7fe98b54cf2c12ab6ba7fac1e1aa7c87e93790c4
    %cd /content/
    !rm KoGPT2/requirements.txt
    !cp PrLM-for-generation-tasks/fairseq-master/kogpt_requirement.txt /KoGPT2/
    !rm KoGPT2/kogpt2/pytorch_kogpt2.py
    !cp PrLM-for-generation-tasks/fairseq-master/pytorch_kogpt2.py KoGPT2/kogpt2/
    %cd KoGPT2
    !pip install -r KoGPT2/kogpt_requirement.txt
    !pip install .
    %cd /content/
    ```

### Toy Tutorial

------



#### Data preprocessing

- First, you should prepare tokenized source and target files for your generation tasks (e.g., Korean-to-English translation):

  ```
  train.src valid.src test.src train.tgt valid.tgt test.tgt

  # !head -n 5 train.src
  # 니 아리 는 딸 이 태어난 이후 줄곧 모유 를 먹여 왔다 . 
  # 도널드 트럼프 미국 대통령 은 21일 ( 현지 시간 ) 북한 이 대형 핵 시험 장 4 곳 을 파괴 했다며 “ 전면 적 비핵화 ( to@@ t@@ al d@@ en@@ uc@@ le@@ ar@@ iz@@ ation ) 가 이미 일어나기 시작 했다 ” 고 말 했다 . 
  # 예상 을 넘어선 폭염 이 가장 큰 변수 로 작용 했다 . 
  # 낙후 지역 이 전 기업 이 공유 재산 의 활용 을 희망 하는 경우 「 공유 재산 및 물품 관리 법 」 및 「 경기도 공유 재산관리 조례 」 가 정 하는 바 에 따라 우선 지원 할 수 있다 . 
  # 우리 얼른 밖 으로 나@@ 가요 , 더 있다가 무슨 일이 일어@@ 날@@ 지도 모르겠어요 . 

  # !head -n 5 train.tgt
  # ni@@ ari has been breastfeeding ever since her daughter was born .
  # u.s. president donald trump said on the 21st ( local time ) that north korea blew up 4 big nuclear test sites and that &quot; a process of total denuclearization has already started . &quot;
  # the heatwave that exceeded expectations served as the biggest variable .
  # if a company relocating to an underdeveloped area intends to utilize public property , it may be preferentially supported as prescribed by the public property and commodity management act and the gyeonggi-do ordinance on public property management .
  # let &apos;s quickly go outside , as i don &apos;t know what else would happen if we stay longer .
  ```

- Then rename original source files (non-tokenized or non-bpe) to `*.prlm` like:

  ```
  train.prlm valid.prlm test.prlm

  # !head -n 5 train.prlm
  # 니아리는 딸이 태어난 이후 줄곧 모유를 먹여왔다.
  # 도널드 트럼프 미국 대통령은 21일(현지시간) 북한이 대형 핵 시험장 4곳을 파괴했다며 “전면적 비핵화(total denuclearization)가 이미 일어나기 시작했다”고 말했다.
  # 예상을 넘어선 폭염이 가장 큰 변수로 작용했다.
  # 낙후지역 이전기업이 공유재산의 활용을 희망하는 경우 「공유재산 및 물품관리법」 및 「경기도 공유재산관리조례」가 정하는 바에 따라 우선 지원할 수 있다.
  # 우리 얼른 밖으로 나가요, 더 있다가 무슨 일이 일어날지도 모르겠어요.
  ```

- These `*.prlm` files are processed through our `preprocess.py` according to each PrLM's configurations (i.e., word segmentation rules, vocab size).

  ```
  TEXT={data path}
  prlmsuffix={pre-trained langauge model you want to use}
  !python /content/Exploiting-PrLM-for-NLG-tasks/fairseq_cli/preprocess.py \
  --source-lang src --target-lang tgt --trainpref $TEXT/train \
  --validpref $TEXT/valid \
  --testpref $TEXT/test \
  --prlm-suffix $prlmsuffix \
  --destdir $TEXT/destdir \
  --joined-dictionary --workers 1
  ```

  - The additional options we add:

    ```
    group.add_argument("--prlm-suffix", metavar="FP", default=None, help="prlm file suffix")
    ```

  - For `--prlm-suffix` , you have 10 options like:

    ```
    KoBERT: "kobert"
    HanBERT: "han_bert"
    DistilKoBERT: "distilkobert"
    KoGPT2: "kogpt2"
    ELECTRA base: "electra_base_v3"
    ELECTRA small: "electra_small_v3"
    Multi-lingual BERT cased: "bert_base_multilingual_cased"
    Multi-lingual BERT uncased: "bert_base_multilingual_uncased"
    XLM RoBERTa base: "xlm_roberta_base"
    XLM RoBERTa large: "xlm_roberta_large" 
    ```
  
  



#### Model Train

- Train a Transformer model with 6 stacked encoder and decoder-layers.

  ```
  !python /content/Exploiting-PrLM-for-NLG-tasks/fairseq_cli/train.py \
      $TEXT/destdir --arch transformer --share-all-embeddings \
      --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --activation-fn relu \
      --lr 0.0007 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-07 \
      --dropout 0.3 --attention-dropout 0.1 --weight-decay 0.0\
      --max-tokens 3500 --label-smoothing 0.1 --save-dir savedir --log-interval 200 --max-epoch 50 \
      --keep-interval-updates -1 --save-interval-updates 0 --criterion label_smoothed_cross_entropy --update-freq 8  \
      --load-prlm --add-prlm $prlmsuffix --merging-method $mergingmethod  --where-add-connection $whereaddconnection \
      --posit-add-prlm $positaddprlm --which-layer-from-prlm $whichlayerfromprlm \
  ```

  - The additional options we add:

    ```
    parser.add_argument('--merging-method', default=None, help='how to merge prlm with original')           
    parser.add_argument('--posit-add-prlm', default=None, help='position of applying prlm')
    parser.add_argument('--which-layer-from-prlm', type=int, default=-1, help='which-layer-from-prlm')
    parser.add_argument('--where-add-connection', default=None, help='--where-add-connection')
    parser.add_argument('--add-prlm', default=None, help='add prlm')
    ```

  - For `--merging-method` , you have 4 options like:

    ```
    Using summation: "add"
    Using gate network: "gate"
    Using average: "average"
    Using dropnet: "dropnet"
    ```

  - For `--posit-add-prlm` , you have 3 options like:

    ```
    Adding PrLM-dedicated modules to encoder: "src"
    Adding PrLM-dedicated modules to decoder: "tgt"
    Adding PrLM-dedicated modules to both: "all"
    ```

  - For `--where-add-connection` , you have 4 options like:

    ```
    Merging the PrLM flow with the source input flows after Self-Attention module: "self"
    Merging the PrLM flow with the source input flows after First Add&Norm module: "self_norm"
    Merging the PrLM flow with the source input flows after Feed-Forward Network module: "ffn"
    Merging the PrLM flow with the source input flows after Second Add&Norm module: "ffn_norm"
    ```

  - For `--which-layer-from-prlm` , you can select the intermediate hidden state of the PrLM. 

    - Example, `-2` denotes second-to-last hidden state(we use this setting), and `-4` denotes forth-to-last hidden state.



#### Generate

- Generate inference using the trained model.

  ```
  !python /content/Exploiting-PrLM-for-NLG-tasks/fairseq_cli/generate.py \
      $TEXT/destdir --gen-subset test --load-prlm --add-prlm $prlmsuffix \
      --source-lang src --target-lang tgt \
      --path $modelcheckpointpath  --beam 5 --nbest 1 --quiet 
  ```

