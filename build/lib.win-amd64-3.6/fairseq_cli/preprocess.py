#!/usr/bin/env python3
# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.
"""
Data pre-processing: build vocabularies and binarize training data.
"""

from collections import Counter
from itertools import zip_longest
import logging
from multiprocessing import Pool
import os
import shutil
import sys

from fairseq import options, tasks, utils
from fairseq.data import indexed_dataset
from fairseq.binarizer import Binarizer
from fairseq.data import Dictionary

logging.basicConfig(
    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    level=logging.INFO,
    stream=sys.stdout,
)
logger = logging.getLogger('fairseq_cli.preprocess')


def main(args):
    utils.import_user_module(args)

    os.makedirs(args.destdir, exist_ok=True)

    logger.addHandler(logging.FileHandler(
        filename=os.path.join(args.destdir, 'preprocess.log'),
    ))
    logger.info(args)

    task = tasks.get_task(args.task)

    def train_path(lang):
        return "{}{}".format(args.trainpref, ("." + lang) if lang else "")

    def file_name(prefix, lang):
        fname = prefix
        if lang is not None:
            fname += ".{lang}".format(lang=lang)
        return fname

    def dest_path(prefix, lang):
        return os.path.join(args.destdir, file_name(prefix, lang))

    def dict_path(lang):
        return dest_path("dict", lang) + ".txt"

    def build_dictionary(filenames, src=False, tgt=False):
        assert src ^ tgt
        return task.build_dictionary(
            filenames,
            workers=args.workers,
            threshold=args.thresholdsrc if src else args.thresholdtgt,
            nwords=args.nwordssrc if src else args.nwordstgt,
            padding_factor=args.padding_factor,
        )

    target = not args.only_source

    if not args.srcdict and os.path.exists(dict_path(args.source_lang)):
        raise FileExistsError(dict_path(args.source_lang))
    if target and not args.tgtdict and os.path.exists(dict_path(args.target_lang)):
        raise FileExistsError(dict_path(args.target_lang))

    if args.joined_dictionary:
        assert not args.srcdict or not args.tgtdict, \
            "cannot use both --srcdict and --tgtdict with --joined-dictionary"

        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        elif args.tgtdict:
            src_dict = task.load_dictionary(args.tgtdict)
        else:
            assert args.trainpref, "--trainpref must be set if --srcdict is not specified"
            src_dict = build_dictionary(
                {train_path(lang) for lang in [args.source_lang, args.target_lang]}, src=True
            )
        tgt_dict = src_dict
    else:
        if args.srcdict:
            src_dict = task.load_dictionary(args.srcdict)
        else:
            assert args.trainpref, "--trainpref must be set if --srcdict is not specified"
            src_dict = build_dictionary([train_path(args.source_lang)], src=True)

        if target:
            if args.tgtdict:
                tgt_dict = task.load_dictionary(args.tgtdict)
            else:
                assert args.trainpref, "--trainpref must be set if --tgtdict is not specified"
                tgt_dict = build_dictionary([train_path(args.target_lang)], tgt=True)
        else:
            tgt_dict = None

    src_dict.save(dict_path(args.source_lang))
    if target and tgt_dict is not None:
        tgt_dict.save(dict_path(args.target_lang))
        
    # print(args.prlm_suffix)
    if args.prlm_suffix == "sktbrain_kobert":
        import gluonnlp as nlp
        import torch
        from kobert.pytorch_kobert import get_pytorch_kobert_model
        from kobert.utils import get_tokenizer
        
        _, prlm_vocab  = get_pytorch_kobert_model()
        prlm_tokenizer = nlp.data.SentencepieceTokenizer(get_tokenizer())
        # prlm_transform = nlp.data.BERTSentenceTransform(prlm_tokenizer, 200, prlm_vocab, pad=True, pair=False)
    
    elif args.prlm_suffix == "electra_base_v1":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "electra_base_v2":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v2-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "electra_base_v3":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "electra_small_v1":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "electra_small_v2":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-v2-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "electra_small_v3":
        # print(args.prlm_suffix)
        from transformers import ElectraTokenizer
        
        prlm_tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-small-v3-discriminator")
        prlm_vocab  = prlm_tokenizer.vocab
        # print(args.prlm_suffix)
        
    elif args.prlm_suffix == "xlm_roberta_base":

        from transformers import AutoTokenizer, AutoModel
        
        prlm_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')
        prlm_vocab  = prlm_tokenizer.vocab
        
    elif args.prlm_suffix == "xlm_roberta_large":

        from transformers import AutoTokenizer, AutoModel
        
        prlm_tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-large')
        prlm_vocab  = prlm_tokenizer.vocab
    
    elif args.prlm_suffix == "bert_base_multilingual_uncased":

        from transformers import AutoTokenizer, AutoModel
        
        prlm_tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-uncased')
        prlm_vocab  = prlm_tokenizer.vocab
        
    elif args.prlm_suffix in ["kobert", "distilkobert"]:
    
        from tokenization_kobert import KoBertTokenizer
        
        prlm_tokenizer = KoBertTokenizer.from_pretrained('monologg/kobert') # monologg/distilkobert도 동일
        prlm_vocab  = prlm_tokenizer.get_vocab()
        
    elif args.prlm_suffix == "han_bert": # transformer 2.2.2
    
        from tokenization_hanbert import HanBertTokenizer
        
        prlm_tokenizer = HanBertTokenizer.from_pretrained('HanBert-54kN-torch')
        prlm_vocab  = dict(prlm_tokenizer.vocab)
        # prlm_tokenizer = None
    elif args.prlm_suffix == "hit_elmo":   
        prlm_tokenizer = None
        prlm_vocab = build_dictionary([args.trainpref + ".hit_elmo", args.trainpref +".hit_elmo", args.trainpref + ".hit_elmo"], src=True)
        prlm_vocab.save(dict_path(args.prlm_suffix))
        
    elif args.prlm_suffix == "gyu_w2v":    
        prlm_tokenizer = None
        prlm_vocab = Dictionary.load(os.path.join("/content/drive/MyDrive/korean_prlm/gyu_w2v/vocab.txt"))
        # print(prlm_vocab, prlm_vocab.__len__(), prlm_vocab.unk_word, prlm_vocab.symbols[:10], prlm_vocab.pad_index, prlm_vocab.unk_index) 
        # # <fairseq.data.dictionary.Dictionary object at 0x7fc66e827780> 30189 <unk> ['<s>', '<pad>', '</s>', '<unk>', '하', '이', 'ㄴ', '의', '는', '다'] 1 3
        prlm_vocab.save(dict_path(args.prlm_suffix))
        

    # if args.prlm_suffix:
        # prlm_dict = build_dictionary(
                # [args.trainpref +"." + args.prlm_suffix], src=True
            # ) 
        # # src -> nwords :defines the total number of words in the final dictionary,
              # # including special symbols
        
        # prlm_dict.save(dict_path("prlm")) 

    def make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers):
        logger.info("[{}] Dictionary: {} types".format(lang, len(vocab)))
        n_seq_tok = [0, 0]
        replaced = Counter()

        def merge_result(worker_result):
            replaced.update(worker_result["replaced"])
            n_seq_tok[0] += worker_result["nseq"]
            n_seq_tok[1] += worker_result["ntok"]

        input_file = "{}{}".format(
            input_prefix, ("." + lang) if lang is not None else ""
        )
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                pool.apply_async(
                    binarize,
                    (
                        args,
                        input_file,
                        vocab,
                        prefix,
                        lang,
                        offsets[worker_id],
                        offsets[worker_id + 1]
                    ),
                    callback=merge_result
                )
            pool.close()

        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, "bin"),
                                          impl=args.dataset_impl, vocab_size=len(vocab))
        merge_result(
            Binarizer.binarize(
                input_file, vocab, lambda t: ds.add_item(t),
                offset=0, end=offsets[1]
            )
        )
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, lang)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))

        ds.finalize(dataset_dest_file(args, output_prefix, lang, "idx"))

        logger.info(
            "[{}] {}: {} sents, {} tokens, {:.3}% replaced by {}".format(
                lang,
                input_file,
                n_seq_tok[0],
                n_seq_tok[1],
                100 * sum(replaced.values()) / n_seq_tok[1],
                vocab.unk_word,
            )
        )
        
    def make_binary_prlm_dataset(vocab, prlm_tokenizer, input_prefix, output_prefix, lang, num_workers):
     
        # input_prefix (e.g., "train.sktbrain_kobert")  output_prefix "train.prlm"
         
        
        logger.info("[{}] Dictionary: {} types".format(args.prlm_suffix,len(vocab)))
        n_seq_tok = [0, 0, 0]
        # replaced = Counter()

        def merge_result(worker_result):
            n_seq_tok[0] += worker_result["nseq"]
            n_seq_tok[1] += worker_result["ntok"]
            n_seq_tok[2] += worker_result["nunk"]
            
        input_file = "{}{}".format(
            input_prefix, ("." + lang) if lang is not None else ""
        )
        # sktbrain_kobert.ko
        
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        
        # print("here")
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                # print("here2", prefix)
                # print("vocab", vocab)
                # print("prlm_tokenizer", prlm_tokenizer)
                test = pool.apply_async(
                    binarize_prlm,
                    (
                        args,
                        input_file,
                        vocab,
                        prlm_tokenizer,
                        prefix,
                        lang,
                        offsets[worker_id],
                        offsets[worker_id + 1]
                    ),
                    callback=merge_result
                )
                

            pool.close()

        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, "bin"),
                                          impl=args.dataset_impl)
        merge_result(
            Binarizer.binarize_prlm(
                input_file, vocab, prlm_tokenizer, lambda t: ds.add_item(t),
                offset=0, end=offsets[1], prlm=args.prlm_suffix
            )
        )
        
        # exit()
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, lang)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))

        ds.finalize(dataset_dest_file(args, output_prefix, lang, "idx"))

        logger.info(
            "[{}] {}: {} sents, {} tokens, {:.3}% replaced by {}".format(
                input_prefix.split('.')[-1],
                input_file,
                n_seq_tok[0],
                n_seq_tok[1],
                100 * n_seq_tok[2] / n_seq_tok[1],
                "[UNK]",
            )
        )

    def make_binary_alignment_dataset(input_prefix, output_prefix, num_workers):
        nseq = [0]

        def merge_result(worker_result):
            nseq[0] += worker_result['nseq']

        input_file = input_prefix
        offsets = Binarizer.find_offsets(input_file, num_workers)
        pool = None
        if num_workers > 1:
            pool = Pool(processes=num_workers - 1)
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                pool.apply_async(
                    binarize_alignments,
                    (
                        args,
                        input_file,
                        utils.parse_alignment,
                        prefix,
                        offsets[worker_id],
                        offsets[worker_id + 1]
                    ),
                    callback=merge_result
                )
            pool.close()

        ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, "bin"),
                                          impl=args.dataset_impl)

        merge_result(
            Binarizer.binarize_alignments(
                input_file, utils.parse_alignment, lambda t: ds.add_item(t),
                offset=0, end=offsets[1]
            )
        )
        if num_workers > 1:
            pool.join()
            for worker_id in range(1, num_workers):
                prefix = "{}{}".format(output_prefix, worker_id)
                temp_file_path = dataset_dest_prefix(args, prefix, None)
                ds.merge_file_(temp_file_path)
                os.remove(indexed_dataset.data_file_path(temp_file_path))
                os.remove(indexed_dataset.index_file_path(temp_file_path))

        ds.finalize(dataset_dest_file(args, output_prefix, None, "idx"))

        logger.info(
            "[alignments] {}: parsed {} alignments".format(
                input_file,
                nseq[0]
            )
        )

    def make_dataset(vocab, input_prefix, output_prefix, lang, num_workers=1):
        if args.dataset_impl == "raw":
            # Copy original text file to destination folder
            output_text_file = dest_path(
                output_prefix + ".{}-{}".format(args.source_lang, args.target_lang),
                lang,
            )
            shutil.copyfile(file_name(input_prefix, lang), output_text_file)
        else:
            make_binary_dataset(vocab, input_prefix, output_prefix, lang, num_workers)

    def make_all(lang, vocab):
        if args.trainpref:
            make_dataset(vocab, args.trainpref, "train", lang, num_workers=args.workers)
        if args.validpref:
            for k, validpref in enumerate(args.validpref.split(",")):
                outprefix = "valid{}".format(k) if k > 0 else "valid"
                make_dataset(vocab, validpref, outprefix, lang, num_workers=args.workers)
        if args.testpref:
            for k, testpref in enumerate(args.testpref.split(",")):
                outprefix = "test{}".format(k) if k > 0 else "test"
                make_dataset(vocab, testpref, outprefix, lang, num_workers=args.workers)

    def make_all_prlm(lang, vocab, transform):

        if args.prlm_suffix == "han_bert": 
            setattr(args, "workers", 1)
        if args.trainpref and os.path.exists(args.trainpref + "." + "prlm"):
            make_binary_prlm_dataset(vocab, transform,args.trainpref + "." + "prlm", "train.prlm", lang=None, num_workers=args.workers)
        if args.validpref and os.path.exists(args.validpref + "." + "prlm"):
            make_binary_prlm_dataset(vocab, transform,args.validpref + "." + "prlm", "valid.prlm", lang=None, num_workers=args.workers)
        if args.testpref and os.path.exists(args.testpref + "." + "prlm"):
            make_binary_prlm_dataset(vocab, transform,args.testpref + "." + "prlm", "test.prlm", lang=None, num_workers=args.workers) 


    def make_all_alignments():
        if args.trainpref and os.path.exists(args.trainpref + "." + args.align_suffix):
            make_binary_alignment_dataset(args.trainpref + "." + args.align_suffix, "train.align", num_workers=args.workers)
        if args.validpref and os.path.exists(args.validpref + "." + args.align_suffix):
            make_binary_alignment_dataset(args.validpref + "." + args.align_suffix, "valid.align", num_workers=args.workers)
        if args.testpref and os.path.exists(args.testpref + "." + args.align_suffix):
            make_binary_alignment_dataset(args.testpref + "." + args.align_suffix, "test.align", num_workers=args.workers)

    make_all(args.source_lang, src_dict)
    if target:
        make_all(args.target_lang, tgt_dict)
    # print(args.prlm_suffix)
    if args.prlm_suffix:
        # print("1")
        make_all_prlm(args.source_lang, prlm_vocab, prlm_tokenizer)
        
    if args.align_suffix:
        make_all_alignments()

    logger.info("Wrote preprocessed data to {}".format(args.destdir))

    if args.alignfile:
        assert args.trainpref, "--trainpref must be set if --alignfile is specified"
        src_file_name = train_path(args.source_lang)
        tgt_file_name = train_path(args.target_lang)
        freq_map = {}
        with open(args.alignfile, "r", encoding='utf-8') as align_file:
            with open(src_file_name, "r", encoding='utf-8') as src_file:
                with open(tgt_file_name, "r", encoding='utf-8') as tgt_file:
                    for a, s, t in zip_longest(align_file, src_file, tgt_file):
                        si = src_dict.encode_line(s, add_if_not_exist=False)
                        ti = tgt_dict.encode_line(t, add_if_not_exist=False)
                        ai = list(map(lambda x: tuple(x.split("-")), a.split()))
                        for sai, tai in ai:
                            srcidx = si[int(sai)]
                            tgtidx = ti[int(tai)]
                            if srcidx != src_dict.unk() and tgtidx != tgt_dict.unk():
                                assert srcidx != src_dict.pad()
                                assert srcidx != src_dict.eos()
                                assert tgtidx != tgt_dict.pad()
                                assert tgtidx != tgt_dict.eos()

                                if srcidx not in freq_map:
                                    freq_map[srcidx] = {}
                                if tgtidx not in freq_map[srcidx]:
                                    freq_map[srcidx][tgtidx] = 1
                                else:
                                    freq_map[srcidx][tgtidx] += 1

        align_dict = {}
        for srcidx in freq_map.keys():
            align_dict[srcidx] = max(freq_map[srcidx], key=freq_map[srcidx].get)

        with open(
                os.path.join(
                    args.destdir,
                    "alignment.{}-{}.txt".format(args.source_lang, args.target_lang),
                ),
                "w", encoding='utf-8'
        ) as f:
            for k, v in align_dict.items():
                print("{} {}".format(src_dict[k], tgt_dict[v]), file=f)


def binarize(args, filename, vocab, output_prefix, lang, offset, end, append_eos=True):
    ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, "bin"),
                                      impl=args.dataset_impl, vocab_size=len(vocab))

    def consumer(tensor):
        ds.add_item(tensor)

    

    res = Binarizer.binarize(filename, vocab, consumer, append_eos=append_eos,
                             offset=offset, end=end)
    ds.finalize(dataset_dest_file(args, output_prefix, lang, "idx"))
    return res

def binarize_prlm(args, filename, vocab, prlm_tokenizer, output_prefix, lang, offset, end):
    # print("here1")
    ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, lang, "bin"),
                                      impl=args.dataset_impl, vocab_size=None)

    def consumer(tensor):
        ds.add_item(tensor)


    # print(filename)
    # print("before", ds)
    res = Binarizer.binarize_prlm(filename, vocab, prlm_tokenizer, consumer, offset=offset, end=end, prlm=args.prlm_suffix)
    # print("after", ds)
    ds.finalize(dataset_dest_file(args, output_prefix, lang, "idx"))
    return res
    

def binarize_alignments(args, filename, parse_alignment, output_prefix, offset, end):
    ds = indexed_dataset.make_builder(dataset_dest_file(args, output_prefix, None, "bin"),
                                      impl=args.dataset_impl, vocab_size=None)

    def consumer(tensor):
        ds.add_item(tensor)

    res = Binarizer.binarize_alignments(filename, parse_alignment, consumer, offset=offset,
                                        end=end)
    ds.finalize(dataset_dest_file(args, output_prefix, None, "idx"))
    return res


def dataset_dest_prefix(args, output_prefix, lang):
    base = "{}/{}".format(args.destdir, output_prefix)
    if lang is not None:
        lang_part = ".{}-{}.{}".format(args.source_lang, args.target_lang, lang)
    elif args.only_source:
        lang_part = ""
    else:
        lang_part = ".{}-{}".format(args.source_lang, args.target_lang)

    return "{}{}".format(base, lang_part)


def dataset_dest_file(args, output_prefix, lang, extension):
    base = dataset_dest_prefix(args, output_prefix, lang)
    return "{}.{}".format(base, extension)


def get_offsets(input_file, num_workers):
    return Binarizer.find_offsets(input_file, num_workers)


def cli_main():
    parser = options.get_preprocessing_parser()
    args = parser.parse_args()
    main(args)


if __name__ == "__main__":
    cli_main()
