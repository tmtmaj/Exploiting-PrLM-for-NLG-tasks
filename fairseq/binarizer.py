# Copyright (c) Facebook, Inc. and its affiliates.
#
# This source code is licensed under the MIT license found in the
# LICENSE file in the root directory of this source tree.

import os
from collections import Counter

from fairseq.tokenizer import tokenize_line
import torch
from fairseq.file_io import PathManager

def safe_readline(f):
    pos = f.tell()
    while True:
        try:
            return f.readline()
        except UnicodeDecodeError:
            pos -= 1
            f.seek(pos)  # search where this character begins


class Binarizer:
    @staticmethod
    def binarize(
        filename,
        dict,
        consumer,
        tokenize=tokenize_line,
        append_eos=True,
        reverse_order=False,
        offset=0,
        end=-1,
        already_numberized=False,
    ):
        nseq, ntok = 0, 0
        replaced = Counter()

        def replaced_consumer(word, idx):
            if idx == dict.unk_index and word != dict.unk_word:
                replaced.update([word])

        with open(PathManager.get_local_path(filename), "r", encoding="utf-8") as f:
            f.seek(offset)
            # next(f) breaks f.tell(), hence readline() must be used
            line = safe_readline(f)
            while line:
                if end > 0 and f.tell() > end:
                    break
                if already_numberized:
                    id_strings = line.strip().split()
                    id_list = [int(id_string) for id_string in id_strings]
                    if reverse_order:
                        id_list.reverse()
                    if append_eos:
                        id_list.append(dict.eos())
                    ids = torch.IntTensor(id_list)
                else:
                    ids = dict.encode_line(
                        line=line,
                        line_tokenizer=tokenize,
                        add_if_not_exist=False,
                        consumer=replaced_consumer,
                        append_eos=append_eos,
                        reverse_order=reverse_order,
                    )
                nseq += 1
                ntok += len(ids)
                # print(ids)
                consumer(ids)
                line = f.readline()
        return {
            "nseq": nseq,
            "nunk": sum(replaced.values()),
            "ntok": ntok,
            "replaced": replaced,
        }
        
    @staticmethod
    def binarize_prlm(filename, dict, tokenizer, consumer, offset=0, end=-1, prlm=None):
        nseq, ntok = 0, 0
        # print(filename.split(".")[-1])
        
        if prlm == "sktbrain_kobert":
            unk_token = dict.unknown_token
            unk_token_idx = dict[unk_token]
            cls_token_idx = dict[dict.cls_token]
            sep_token_idx = dict[dict.sep_token]
        elif prlm == "kogpt2":
            unk_token = '<unk>'
            unk_token_idx = dict[unk_token]
        elif prlm in ["electra_base_v1", "electra_base_v2" , "electra_base_v3", "electra_small_v1", "electra_small_v2" , "electra_small_v3"]: #('[PAD]', 0), ('[UNK]', 1),  ('[CLS]', 2), ('[SEP]', 3), ('[MASK]', 4),
            unk_token = "[UNK]"
            unk_token_idx = dict["[UNK]"]
            cls_token_idx = dict["[CLS]"]
            sep_token_idx = dict["[SEP]"]
            pad_token_idx = dict["[PAD]"]
        elif prlm in ["bert_base_multilingual_cased"]: #'[UNK]': 100, '[PAD]': 0, '[CLS]': 101, '[SEP]': 102, '[MASK]':103
            unk_token = "[UNK]"
            unk_token_idx = dict["[UNK]"]
            cls_token_idx = dict["[CLS]"]
            sep_token_idx = dict["[SEP]"]
            pad_token_idx = dict["[PAD]"]
        elif prlm in ["kobert", "distilkobert", "bert_base_multilingual_uncased"]: #'[UNK]': 0, '[PAD]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]':4
            unk_token = "[UNK]"
            unk_token_idx = dict["[UNK]"]
            cls_token_idx = dict["[CLS]"]
            sep_token_idx = dict["[SEP]"]
            pad_token_idx = dict["[PAD]"]
        elif prlm in ["xlm_roberta_base", "xlm_roberta_large"]: #pad 1
            unk_token = tokenizer.unk_token
            unk_token_idx = tokenizer.unk_token_id
            pad_token_idx = tokenizer.pad_token_id
        elif prlm == "hit_elmo":
            unk_token = "None"
            unk_token_idx = -1
        elif prlm == "gyu_w2v":
            # print(dict.unk_word, dict.pad_word, dict.unk_index, dict.pad_index) # <unk> <pad> 3 1
            unk_token = dict.unk_word
            unk_token_idx = dict.unk_index
            pad_token_idx = dict.pad_index
        elif prlm == "han_bert": #'[UNK]': 2, '[PAD]': 0, '[UNK]', 2, '[CLS]': 3, '[SEP]': 4, '[MASK]':5
            unk_token = "[UNK]"
            unk_token_idx = dict["[UNK]"]
            cls_token_idx = dict["[CLS]"]
            sep_token_idx = dict["[SEP]"]
            pad_token_idx = dict["[PAD]"]
            
        replaced = 0
        with open(PathManager.get_local_path(filename), "r", encoding="utf-8") as f: # "train.sktbrain_kobert"
            f.seek(offset)
            # next(f) breaks f.tell(), hence readline() must be used
            line = safe_readline(f)
            while line:
                if end > 0 and f.tell() > end:
                    break
                # tensor_token_ids = torch.IntTensor([cls_token_idx] + dict.to_indices(tokenizer(line)) +[sep_token_idx])
                if prlm == "sktbrain_kobert":
                    tensor_token_ids = torch.IntTensor(dict.to_indices(tokenizer(line.strip())))
                elif prlm in ["electra_base_v1", "electra_base_v2" , "electra_base_v3", "electra_small_v1", "electra_small_v2" , "electra_small_v3", "kobert", "distilkobert", "xlm_roberta_base", "xlm_roberta_large", "bert_base_multilingual_uncased", "bert_base_multilingual_cased"]:
                    tensor_token_ids = tokenizer(line.strip(), return_tensors="pt")['input_ids'].squeeze()
                elif prlm == "hit_elmo":
                    tensor_token_ids = dict.encode_line(
                        line=line,
                        line_tokenizer=tokenize_line,
                        add_if_not_exist=False,
                        consumer=None,
                        append_eos=False,
                        reverse_order=False,
                    )
                elif prlm == "gyu_w2v":
                    tensor_token_ids = dict.encode_line(
                        line=line,
                        line_tokenizer=tokenize_line,
                        add_if_not_exist=False,
                        consumer=None,
                        append_eos=False,
                        reverse_order=False,
                    )
                elif prlm == "han_bert":
                    tensor_token_ids = torch.IntTensor(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(line.strip())))
                elif prlm == "kogpt2":
                    tensor_token_ids = torch.IntTensor(dict[tokenizer(line.strip())])
                # print(dict[tokenizer(line.strip())])
                # print(tensor_token_ids)
                # exit()

                    
                # print(line)
                # print(" ".join(tokenizer.convert_ids_to_tokens(tensor_token_ids.tolist())))
                # exit()
                if tensor_token_ids.size(0) is not None and tensor_token_ids.size(0) > 512:
                  tensor_token_ids = torch.cat([tensor_token_ids[:256], tensor_token_ids[-256:]], -1)
                  
                nseq += 1
                replaced += sum(tensor_token_ids.eq(unk_token_idx))
                ntok += len(tensor_token_ids) 
                consumer(tensor_token_ids)
                    
                
                # print(tensor_token_ids)
                line = f.readline()
        return {
            "nseq": nseq,
            "nunk": replaced,
            "ntok": ntok,
            "replaced": unk_token,
        }
    @staticmethod
    def binarize_alignments(filename, alignment_parser, consumer, offset=0, end=-1):
        nseq = 0

        with open(PathManager.get_local_path(filename), "r") as f:
            f.seek(offset)
            line = safe_readline(f)
            while line:
                if end > 0 and f.tell() > end:
                    break
                ids = alignment_parser(line)
                nseq += 1
                consumer(ids)
                line = f.readline()
        return {"nseq": nseq}

    @staticmethod
    def find_offsets(filename, num_chunks):
        with open(PathManager.get_local_path(filename), "r", encoding="utf-8") as f:
            size = os.fstat(f.fileno()).st_size
            chunk_size = size // num_chunks
            offsets = [0 for _ in range(num_chunks + 1)]
            for i in range(1, num_chunks):
                f.seek(chunk_size * i)
                safe_readline(f)
                offsets[i] = f.tell()
            return offsets
